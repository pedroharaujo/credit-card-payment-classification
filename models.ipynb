{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9yvEF05tzIXy"
      },
      "source": [
        "## Machine Learning for Credit Card Payment Classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9A3SosdfRedT"
      },
      "source": [
        "## Introduction\n",
        "This work aims to apply some ensemble techniques and carry out a comparative study in order to classify customers in relation to credit card payments. The data used here were obtained from kaggle and are provided by the UCI, originally presenting 30000 observations of 24 variables.\n",
        "\n",
        "The models that will be fitted and compared are:\n",
        "- Decision Tree;\n",
        "- Random Forest;\n",
        "- AdaBossting;\n",
        "- Gradient Boosting.\n",
        "\n",
        "For parameters selection, we applied the cross-validation procedure and after choosing the best value of the parameters, the performance of each model was evaluated by means of the metrics of accuracy, precision and recall for the test and training cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Y8AwNDlebAzl",
        "outputId": "20bf7850-3920-47e9-e72d-7ea943ef5001"
      },
      "outputs": [],
      "source": [
        "import plotly.offline as pyo\n",
        "import plotly.graph_objs as go\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib as mpl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from keras import datasets\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import models\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def printmd(s):\n",
        "    display(Markdown(s))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MBG1Yz8wRiVc"
      },
      "source": [
        "Reading data using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqe47abyDKpK"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "data = pd.read_csv('./UCI_Credit_Card.csv', index_col=0)\n",
        "data = data.reset_index(drop=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "17VxuygwRpD6"
      },
      "source": [
        "Originally, the data presented categorical and numerical variables. The code below extracts the columns corresponding to each type of variable. The categorical variables are defined as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2ixZuCsMC84"
      },
      "outputs": [],
      "source": [
        "# creating lists based on types of variables\n",
        "df = data\n",
        "numcols = df.dtypes[df.dtypes == 'float64'].index\n",
        "Nnumcols = df.dtypes[df.dtypes != 'float64'].index"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5QOWpKc2RsZ1"
      },
      "source": [
        "For numerical variables, data were normalized with aiming to reduce the computational cost without losing the order of magnitude of the values of each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLnurpLUOyNW"
      },
      "outputs": [],
      "source": [
        "def normalization(x):\n",
        "    return (x-x.mean())/x.std()\n",
        "\n",
        "ndf = normalization(df[numcols.tolist() + ['AGE']])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tLUvnCTKRukD"
      },
      "source": [
        "For each categorical variable, the one-hot encoding technique is applied, which consists of transforming the $n$ classes of categorical variables into $n$ binary variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tScSLYq-RTmX"
      },
      "outputs": [],
      "source": [
        "# Dropping non categorical columns - and y column\n",
        "cats = data[Nnumcols].drop(['AGE', 'default.payment.next.month'], axis=1).columns\n",
        "\n",
        "# One-hot encoding categorical variables\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "ohe_df = ohe.fit_transform(df[cats])\n",
        "ohe_df = pd.DataFrame(ohe_df, columns=ohe.get_feature_names_out())\n",
        "\n",
        "# Joining formatted data\n",
        "data = pd.concat([df[['default.payment.next.month']], ohe_df, ndf], axis=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VwrLZ8XuR2fF"
      },
      "source": [
        "We separate the variable of interest (y) from the covariates (X)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSTUu2zBCGcK"
      },
      "outputs": [],
      "source": [
        "# X and y spllited\n",
        "X = data.drop('default.payment.next.month', axis=1)\n",
        "y = data['default.payment.next.month']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UMMHpsWWSAMR"
      },
      "source": [
        "Calculating the percentage of each of the response variable classes, where 1 is equivalent to the 'Default' class and 0 to 'Paid', it can be seen that we are dealing with a problem of unbalanced data. For this reason, all models were adjusted with weights inversely proportional to the probability of each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "eIdqCq028bwj",
        "outputId": "f8adc431-3fcc-4260-aa9d-046620832cd4"
      },
      "outputs": [],
      "source": [
        "print(len(y[y==1])/len(y))\n",
        "print(len(y[y==0])/len(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmXZdTFjP6-c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Generic function for Cross-Validation\n",
        "def generate_model(classifier, X, y, params, n_splits):\n",
        "  cv = KFold(n_splits=n_splits, random_state=1, shuffle=True)\n",
        "  acc_test_mean = []\n",
        "  acc_train_mean = []\n",
        "  prec_test_mean = []\n",
        "  prec_train_mean = []\n",
        "  rec_test_mean = []\n",
        "  rec_train_mean = []\n",
        "  for par in params:\n",
        "    acc_test = []\n",
        "    acc_train = []\n",
        "    prec_train = []\n",
        "    prec_test = []\n",
        "    rec_train = []\n",
        "    rec_test = []\n",
        "    for train_index, test_index in cv.split(X):\n",
        "      # train test split\n",
        "      X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
        "      y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "      if classifier == 'RF':\n",
        "        model = RandomForestClassifier(n_estimators=par, max_depth=3, class_weight='balanced',\n",
        "                                       random_state=0)\n",
        "      elif classifier == 'Gboost':\n",
        "        model = GradientBoostingClassifier(n_estimators=par, random_state=0)\n",
        "      elif classifier == 'DT':\n",
        "        model = DecisionTreeClassifier(max_depth=par, class_weight='balanced')\n",
        "      elif classifier == 'adaboost':\n",
        "        model = AdaBoostClassifier(n_estimators=par)\n",
        "\n",
        "      # model training\n",
        "      model.fit(X_train, y_train)\n",
        "\n",
        "      # predictions\n",
        "      y_pred_test = model.predict(X_test)\n",
        "      y_pred_train = model.predict(X_train)\n",
        "\n",
        "      # accuracy, precision and recall\n",
        "      acc_test.append(metrics.accuracy_score(y_test, y_pred_test))\n",
        "      acc_train.append(metrics.accuracy_score(y_train, y_pred_train))\n",
        "\n",
        "      prec_train.append(metrics.precision_score(y_train, y_pred_train))\n",
        "      prec_test.append(metrics.precision_score(y_test, y_pred_test))\n",
        "\n",
        "      rec_train.append(metrics.recall_score(y_train, y_pred_train))\n",
        "      rec_test.append(metrics.recall_score(y_test, y_pred_test))\n",
        "\n",
        "    # average evaluation metrics\n",
        "    acc_test_mean.append(np.array(acc_test).mean())\n",
        "    acc_train_mean.append(np.array(acc_train).mean())\n",
        "\n",
        "    prec_test_mean.append(np.array(prec_test).mean())\n",
        "    prec_train_mean.append(np.array(prec_train).mean())\n",
        "\n",
        "    rec_test_mean.append(np.array(rec_test).mean())\n",
        "    rec_train_mean.append(np.array(rec_train).mean())\n",
        "\n",
        "\n",
        "  return acc_train_mean, acc_test_mean, prec_train_mean, prec_test_mean, rec_train_mean, rec_test_mean"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a6p2hKyKSeO2"
      },
      "source": [
        "We split the data into training and test sets so that we could evaluate the model on a data set that was not presented to the model during training. Thus, we can measure the generalization power of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVzxjD7vXGky"
      },
      "outputs": [],
      "source": [
        "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X, y, test_size=0.4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TOkXQuJL8SRy"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iy745E_Sl7A"
      },
      "source": [
        "The first method to be tested is decision tree, where the variable parameter was the maximum depth of the tree. In this work, I will not go into details about the operation and/or the theory behind the methods.\n",
        "\n",
        "Cross-validation is performed for decision trees and the behavior graph of training and test accuracy is used to choose the best parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKZ7uPXk8hW9"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Cross Validation\n",
        "params = [2, 3, 4, 5, 6, 8, 10]\n",
        "acc_train_mean, acc_test_mean, prec_train_mean, prec_test_mean, rec_train_mean, rec_test_mean = generate_model(classifier='DT', X=X, y=y, params=params, n_splits=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "C5dRhH2GStcj",
        "outputId": "ab409d4f-904c-43ee-80f4-1db3e84824e1"
      },
      "outputs": [],
      "source": [
        "# evaluation metrics dataframe creation\n",
        "acc_mean_test = pd.DataFrame(acc_test_mean, index=params, columns=['acc_mean_test'])\n",
        "acc_mean_train = pd.DataFrame(acc_train_mean, index=params, columns=['acc_mean_train'])\n",
        "prec_mean_test = pd.DataFrame(prec_test_mean, index=params, columns=['prec_mean_test'])\n",
        "prec_mean_train = pd.DataFrame(prec_train_mean, index=params, columns=['prec_mean_train'])\n",
        "rec_mean_test = pd.DataFrame(rec_test_mean, index=params, columns=['rec_mean_test'])\n",
        "rec_mean_train = pd.DataFrame(rec_train_mean, index=params, columns=['rec_mean_train'])\n",
        "\n",
        "\n",
        "results = pd.DataFrame(pd.concat([acc_mean_train, acc_mean_test,\n",
        "                                  prec_mean_train, prec_mean_test,\n",
        "                                  rec_mean_train, rec_mean_test,], axis=1))\n",
        "\n",
        "# CV results plot\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Tree Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Decision Tree Cross-Validation')\n",
        "plt.plot(results['acc_mean_train'], label='Train')\n",
        "plt.plot(results['acc_mean_test'], label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hf7KyeFITJWN"
      },
      "source": [
        "As we can see, the best values for the accuracy is found when using 2 or 3 as the DT model parameter. Since we want our model to be as complex as it can be without losing accuracy, we select 3 as the oficial parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "e7R2tYuBg1k2",
        "outputId": "9746b919-010a-49b0-e866-2c01b0bd9807"
      },
      "outputs": [],
      "source": [
        "for i in params:\n",
        "    print(\"RESULTS FOR TREE DEPTH {0}\".format(i))\n",
        "    print(results.loc[i])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "szfzPRrC8rnh",
        "outputId": "fe4bb2bc-007e-41f1-cd2f-1dafdb921b1d"
      },
      "outputs": [],
      "source": [
        "# Average ROC curve plot\n",
        "from scipy import interp\n",
        "plt.figure(figsize=(15,10))\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "classifier = DecisionTreeClassifier(max_depth=6)\n",
        "model = classifier\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "for i, (train, test) in enumerate(cv.split(X)):\n",
        "    probas_ = classifier.fit(X.loc[train], y.loc[train]).predict_proba(X.loc[test])\n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y.loc[test], probas_[:, 1])\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve DT')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FGk7MThCTxwo"
      },
      "source": [
        "With the graph of the ROC curve we can see that, on average, our decision tree is able to separate a large percentage of the data relatively well. However, when analyzing the confusion matrix below, it is noted that the accuracy is high, but unbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "KbfS48QWSsff",
        "outputId": "a7720780-acb8-40d9-c617-bd5d7a1450b6"
      },
      "outputs": [],
      "source": [
        "# Decision Tree Confusion Matrix\n",
        "y_pred = model.predict(X_TEST)\n",
        "\n",
        "# Labels\n",
        "y_true = Y_TEST\n",
        "\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "classes = list(range(0, 2))\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "ax.set(xticks=(0, 1),\n",
        "       yticks=(0, 1),\n",
        "       xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "ax.set_title(\"Decision Tree Confusion Matrix\", fontsize=16)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\", size= 12)\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), size= 12)\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], \"d\"),\n",
        "                ha=\"center\", va=\"center\", fontsize=14,\n",
        "                color=\"white\" if cm[i, j] > cm.max()/2. else \"black\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R0rwY_uiUD81"
      },
      "source": [
        "Therefore, we conclude for this model that even applying the weights to the classes, the method still has difficulty in correctly classifying cases of the minority class (\"Default\"). This means that our model is bad at figuring out who won't pay the credit card bill the following month, but it's good at figuring out who will pay."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMu2Z1N38OQz"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yk35p9zyUmgu"
      },
      "source": [
        "The same methodology applied to the decision tree was applied to the next methods. Therefore, the interpretation part of each step will not be explored in a peculiar way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSb4DDm4P678"
      },
      "outputs": [],
      "source": [
        "# Random Forest Cross Validation\n",
        "params = list(range(5, 50, 5))\n",
        "acc_train_mean, acc_test_mean, prec_train_mean, prec_test_mean, rec_train_mean, rec_test_mean = generate_model(classifier='RF', X=X, y=y.values.ravel(), params=params, n_splits=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "e92I-AF8P65j",
        "outputId": "0814295f-6978-47f5-cda4-7df16de681e7"
      },
      "outputs": [],
      "source": [
        "# Dataframes results\n",
        "acc_mean_test = pd.DataFrame(acc_test_mean, index=params, columns=['acc_mean_test'])\n",
        "acc_mean_train = pd.DataFrame(acc_train_mean, index=params, columns=['acc_mean_train'])\n",
        "prec_mean_test = pd.DataFrame(prec_test_mean, index=params, columns=['prec_mean_test'])\n",
        "prec_mean_train = pd.DataFrame(prec_train_mean, index=params, columns=['prec_mean_train'])\n",
        "rec_mean_test = pd.DataFrame(rec_test_mean, index=params, columns=['rec_mean_test'])\n",
        "rec_mean_train = pd.DataFrame(rec_train_mean, index=params, columns=['rec_mean_train'])\n",
        "\n",
        "\n",
        "results = pd.DataFrame(pd.concat([acc_mean_train, acc_mean_test,\n",
        "                                  prec_mean_train, prec_mean_test,\n",
        "                                  rec_mean_train, rec_mean_test,], axis=1))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Número da Árvore')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.title('Cross-Validation para Random Forest')\n",
        "plt.plot(results['acc_mean_train'], label='Train')\n",
        "plt.plot(results['acc_mean_test'], label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "k3E-t8MQhY-8",
        "outputId": "3752e952-2ed4-4f42-ee40-0ed1d8349bdc"
      },
      "outputs": [],
      "source": [
        "for i in params:\n",
        "    print(\"RESULTS FOR RANDOM FOREST NUMBER OF ESTIMATORS {0}\".format(i))\n",
        "    print(results.loc[i])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "-IAfURYOP622",
        "outputId": "db19b783-d5ef-4188-80d2-70403fdbcabc"
      },
      "outputs": [],
      "source": [
        "# Curva ROC média para RF\n",
        "plt.figure(figsize=(15,10))\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "classifier = RandomForestClassifier(n_estimators=40, max_depth=6)\n",
        "model = classifier\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "for i, (train, test) in enumerate(cv.split(X)):\n",
        "    probas_ = classifier.fit(X.loc[train], y.loc[train]).predict_proba(X.loc[test])\n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y.loc[test], probas_[:, 1])\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Random Forest')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "lC9seoViV15v",
        "outputId": "f76ee33f-80f4-48b1-eef6-2dac0eff8895"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred = model.predict(X_TEST)\n",
        "\n",
        "# Labels\n",
        "y_true = Y_TEST\n",
        "\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "classes = list(range(0, 2))\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "ax.set(xticks=(0, 1),\n",
        "       yticks=(0, 1),\n",
        "       xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "ax.set_title(\"Random Forest Confusion Matrix\", fontsize=16)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\", size= 12)\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), size= 12)\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], \"d\"),\n",
        "                ha=\"center\", va=\"center\", fontsize=14,\n",
        "                color=\"white\" if cm[i, j] > cm.max()/2. else \"black\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZKMgH8U6d9"
      },
      "source": [
        "We can observe that the accuracy for Random Forests was slightly lower to that found with only one decision tree. However, the confusion matrix reveals that the imbalance problem becomes more evident. Our model manages to be superior for targeting customers who will pay the card, but inferior for the opposite cases.\n",
        "As customers who do not pay bills are responsible for financial institutions' losses, our model may be considered bad by some of these institutions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XJg4K-nq-WzU"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQkaowZWVgMv"
      },
      "source": [
        "For the AdaBoost method, weights were not used as in the others. This is because this method assigns weights to wrong classifications at each iteration. As we have seen that the method to be ensembled (decision trees) has difficulty classifying minority classes, it is expected that AdaBoost will naturally assign greater weights to this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgMB840T-ZFR"
      },
      "outputs": [],
      "source": [
        "# AdaBoosting Cross Validation\n",
        "params = list(range(2, 15, 1))\n",
        "acc_train_mean, acc_test_mean, prec_train_mean, prec_test_mean, rec_train_mean, rec_test_mean = generate_model(classifier='adaboost', X=X, y=y.values.ravel(), params=params, n_splits=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ySmw6LLf-ZCE",
        "outputId": "413a1ff5-10c7-4243-d314-bfef9c4aead0"
      },
      "outputs": [],
      "source": [
        "# Results DataFrame\n",
        "acc_mean_test = pd.DataFrame(acc_test_mean, index=params, columns=['acc_mean_test'])\n",
        "acc_mean_train = pd.DataFrame(acc_train_mean, index=params, columns=['acc_mean_train'])\n",
        "prec_mean_test = pd.DataFrame(prec_test_mean, index=params, columns=['prec_mean_test'])\n",
        "prec_mean_train = pd.DataFrame(prec_train_mean, index=params, columns=['prec_mean_train'])\n",
        "rec_mean_test = pd.DataFrame(rec_test_mean, index=params, columns=['rec_mean_test'])\n",
        "rec_mean_train = pd.DataFrame(rec_train_mean, index=params, columns=['rec_mean_train'])\n",
        "\n",
        "results = pd.DataFrame(pd.concat([acc_mean_train, acc_mean_test,\n",
        "                                  prec_mean_train, prec_mean_test,\n",
        "                                  rec_mean_train, rec_mean_test,], axis=1))\n",
        "\n",
        "# Visual Results\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Number of Trees', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.title('AdaBoost Cross-Validation', fontsize=16)\n",
        "plt.plot(results['acc_mean_train'], label='Train')\n",
        "plt.plot(results['acc_mean_test'], label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "Vz-LZUD6hblt",
        "outputId": "2b4fa762-46f7-46a0-f39b-58985144af0f"
      },
      "outputs": [],
      "source": [
        "for i in params:\n",
        "    print(\"RESULTS FOR ADA BOOSTING NUMBER OF TREES {0}\".format(i))\n",
        "    print(results.loc[i])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "PM6K7qO4-Y_B",
        "outputId": "221d082f-e527-4c5b-b3a6-0cdbadc6f6d6"
      },
      "outputs": [],
      "source": [
        "# Average ROC Curve for Adaboost\n",
        "plt.figure(figsize=(15,10))\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "classifier = AdaBoostClassifier(n_estimators=5, base_estimator=DecisionTreeClassifier(max_depth=3, class_weight='balanced'))\n",
        "model = classifier\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "for i, (train, test) in enumerate(cv.split(X)):\n",
        "    probas_ = classifier.fit(X.loc[train], y.loc[train]).predict_proba(X.loc[test])\n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y.loc[test], probas_[:, 1])\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve AdaBoost', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "NYmiFinpV4TJ",
        "outputId": "5897572a-2248-47e1-99b3-62cc01612fe3"
      },
      "outputs": [],
      "source": [
        "# Model predictions\n",
        "y_pred = model.predict(X_TEST)\n",
        "\n",
        "# Labels\n",
        "y_true = Y_TEST\n",
        "\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "classes = list(range(0, 2))\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "ax.set(xticks=(0, 1),\n",
        "       yticks=(0, 1),\n",
        "       xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "ax.set_title(\"AdaBoost Confusion Matrix\", fontsize=16)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\", size= 12)\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), size= 12)\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], \"d\"),\n",
        "                ha=\"center\", va=\"center\", fontsize=14,\n",
        "                color=\"white\" if cm[i, j] > cm.max()/2. else \"black\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DJCQdZ_iWLvL"
      },
      "source": [
        "Note that AdaBoost presents a significant improvement in accuracy relative to the minority class, as expected. However, getting more cases of that class right causes the model to miss more classifications for the other class.\n",
        "\n",
        "Despite this trade-off, the overall accuracy of the model was slightly higher than the value found for the previous methods."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sj3ttyZkY8GJ"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "p2pGSnF_WpCL"
      },
      "source": [
        "Parameters for Gradient Boosting Cross Validation procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "807YhFZgY-iA",
        "outputId": "2b01a785-4f24-43e6-c60f-3107a90e693d"
      },
      "outputs": [],
      "source": [
        "params = list(range(2, 15, 1))\n",
        "acc_train_mean, acc_test_mean, prec_train_mean, prec_test_mean, rec_train_mean, rec_test_mean = generate_model(classifier='Gboost', X=X, y=y.values.ravel(), params=params, n_splits=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "iQtfqUjvY-cY",
        "outputId": "841c659a-96f9-4d66-b398-eaa6d32c4a96"
      },
      "outputs": [],
      "source": [
        "# Average ROC Curve\n",
        "plt.figure(figsize=(15,10))\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "classifier = GradientBoostingClassifier(n_estimators=10)\n",
        "model = classifier\n",
        "\n",
        "tprs = []\n",
        "aucs = []\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "for i, (train, test) in enumerate(cv.split(X)):\n",
        "    probas_ = classifier.fit(X.loc[train], y.loc[train]).predict_proba(X.loc[test])\n",
        "    # Compute ROC curve and area the curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y.loc[test], probas_[:, 1])\n",
        "    tprs.append(np.interp(mean_fpr, fpr, tpr))\n",
        "    tprs[-1][0] = 0.0\n",
        "    roc_auc = metrics.auc(fpr, tpr)\n",
        "    aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=1, alpha=0.3,\n",
        "             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
        "         label='Chance', alpha=.8)\n",
        "\n",
        "mean_tpr = np.mean(tprs, axis=0)\n",
        "mean_tpr[-1] = 1.0\n",
        "mean_auc = metrics.auc(mean_fpr, mean_tpr)\n",
        "std_auc = np.std(aucs)\n",
        "plt.plot(mean_fpr, mean_tpr, color='b',\n",
        "         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
        "         lw=2, alpha=.8)\n",
        "\n",
        "std_tpr = np.std(tprs, axis=0)\n",
        "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
        "                 label=r'$\\pm$ 1 std. dev.')\n",
        "\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve Gradient Boosting', fontsize=16)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "4WifFrMOY-Z5",
        "outputId": "b973447f-dae3-4f4c-c8d1-c97d466d1eb6"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_TEST)\n",
        "y_true = Y_TEST\n",
        "\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "classes = list(range(0, 2))\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "ax.figure.colorbar(im, ax=ax)\n",
        "ax.set(xticks=(0, 1),\n",
        "       yticks=(0, 1),\n",
        "       xticklabels=classes, yticklabels=classes)\n",
        "\n",
        "ax.set_title(\"Gradient Boosting Confusion Matrix\", fontsize=16)\n",
        "ax.set_xlabel('Predicted', fontsize=14)\n",
        "ax.set_ylabel('Real', fontsize=14)\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\", size= 12)\n",
        "\n",
        "plt.setp(ax.get_yticklabels(), size= 12)\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, format(cm[i, j], \"d\"),\n",
        "                ha=\"center\", va=\"center\", fontsize=14,\n",
        "                color=\"white\" if cm[i, j] > cm.max()/2. else \"black\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G6CTzCnKWxyU"
      },
      "source": [
        "We conclude that the Gradient Tree Boosting method presented results similar to those found by the first 2 methods. Therefore, the analysis to be done here follows the same logical concept applied to such methods."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oeNdkC9dXGlW"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tIL4y7qoXMDI"
      },
      "source": [
        "At the end of this propject, it was possible to observe the behavior of some ensemble Machine Learning methods for a dataset with unbalanced classes.\n",
        "\n",
        "After testing 4 different methods, the AdaBoost method showed the greatest ability to distinguish between classes, due to the natural attribution of weights to erroneously classified cases.\n",
        "\n",
        "As a proposal for future work, there is the possibility of testing the behavior of this problem with other classifiers, such as SVM and KNN. Furthermore, it is possible to perform a comparison with the same classifiers for other techniques for unbalanced problems besides weighting."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
